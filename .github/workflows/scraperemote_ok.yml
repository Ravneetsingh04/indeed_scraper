name: Scrape RemoteOK Jobs Daily

on:
  schedule:
    - cron: '45 10 * * *'  # Runs daily at 10:45 AM UTC (after other spiders)
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape_remoteok:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install scrapy requests pandas

      - name: Run Scrapy spider and merge results
        env:
          SCRAPER_API_KEY: ${{ secrets.SCRAPER_API_KEY }}
        working-directory: indeed_scraper
        run: |
          echo "Running RemoteOK spider..."
          mkdir -p $GITHUB_WORKSPACE/output
          
          scrapy crawl remoteok -o $GITHUB_WORKSPACE/output/new_remoteok.csv

          echo "Scrapy completed. Checking output:"
          ls -lh $GITHUB_WORKSPACE/output/
          head -n 10 $GITHUB_WORKSPACE/output/new_remoteok.csv || echo "File empty!"

          cd $GITHUB_WORKSPACE
          if [ -f remoteok_jobs.csv ]; then
            echo "Merging old and new RemoteOK job listings (new on top)..."
            head -n 1 output/new_remoteok.csv > merged_remoteok.csv
            tail -n +2 output/new_remoteok.csv >> merged_remoteok.csv
            tail -n +2 remoteok_jobs.csv >> merged_remoteok.csv
            mv merged_remoteok.csv remoteok_jobs.csv
          else
            echo "No existing file found, creating a new one..."
            cp output/new_remoteok.csv remoteok_jobs.csv
          fi

      - name: Commit and push results
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git add remoteok_jobs.csv || echo "No file to add"
          git commit -m "Append new RemoteOK jobs" || echo "No changes to commit"
          git push
