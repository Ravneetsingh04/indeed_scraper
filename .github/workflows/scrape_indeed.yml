name: Scrape Indeed Jobs Daily

on:
  schedule:
    - cron: '0 10 * * *'  # Runs every day at 10:00 AM UTC
  workflow_dispatch:

permissions:
  contents: write  # Allows workflow to push CSV results back to repo

jobs:
  scrape_jobs:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install scrapy requests

      - name: Run Scrapy spider
        env:
          SCRAPER_API_KEY: ${{ secrets.SCRAPER_API_KEY }}
        working-directory: indeed_scraper
        run: |
          echo "Running Scrapy with API key..."
          mkdir -p $GITHUB_WORKSPACE/output
          scrapy crawl indeed -o $GITHUB_WORKSPACE/output/indeed_jobs.csv
          echo "Scrapy completed. Checking output:"
          ls -lh $GITHUB_WORKSPACE/output/
          head -n 10 $GITHUB_WORKSPACE/output/indeed_jobs.csv || echo "File empty!"

      - name: Commit and push results
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          cp output/indeed_jobs.csv .
          git add indeed_jobs.csv || echo "No file to add"
          git commit -m "Automated update of Indeed jobs" || echo "No changes to commit"
          git push
