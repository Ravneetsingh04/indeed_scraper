name: Scrape WeWorkRemotely Jobs Daily

on:
  schedule:
    - cron: '0 10 * * *'  # Runs every day at 10:00 AM UTC
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape_wwr:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install scrapy requests pandas

      - name: Run Scrapy spider and merge results
        env:
          SCRAPER_API_KEY: ${{ secrets.SCRAPER_API_KEY }}
        working-directory: indeed_scraper
        run: |
          echo "Running WeWorkRemotely spider..."
          mkdir -p $GITHUB_WORKSPACE/output
          
          # Run spider to a temporary file
          scrapy crawl weworkremotely -o $GITHUB_WORKSPACE/output/new_wwr.csv

          echo "Scrapy completed. Checking output:"
          ls -lh $GITHUB_WORKSPACE/output/
          head -n 10 $GITHUB_WORKSPACE/output/new_wwr.csv || echo "File empty!"

          # Merge with existing CSV (new on top)
          cd $GITHUB_WORKSPACE
          if [ -f wwr_jobs.csv ]; then
            echo "Merging old and new WWR job listings (new on top)..."
            head -n 1 output/new_wwr.csv > merged_wwr.csv
            tail -n +2 output/new_wwr.csv >> merged_wwr.csv
            tail -n +2 wwr_jobs.csv >> merged_wwr.csv
            mv merged_wwr.csv wwr_jobs.csv
          else
            echo "No existing file found, creating a new one..."
            cp output/new_wwr.csv wwr_jobs.csv
          fi

      - name: Commit and push results
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git add wwr_jobs.csv || echo "No file to add"
          git commit -m "Append new WWR jobs" || echo "No changes to commit"
          git push
